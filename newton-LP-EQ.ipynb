{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\min -(\\sum_i^n \\log{ x_i}-e^{x_i}) \\quad\\text{subject to}\\quad Ax=b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T14:07:52.216828Z",
     "start_time": "2023-04-27T14:07:52.175512Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T14:07:52.817712Z",
     "start_time": "2023-04-27T14:07:52.797653Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the seed\n",
    "# np.random.seed()\n",
    "m = 500\n",
    "n = 1000\n",
    "\n",
    "A = np.random.randn(m, n)\n",
    "x0 = np.random.random(n)\n",
    "b = A.dot(x0)\n",
    "\n",
    "# c = np.random.randn(n)\n",
    "\n",
    "f = lambda x: -np.sum(np.log(x)-np.exp(x))\n",
    "nabla_f = lambda x: np.diag(1/x**2+np.exp(x))\n",
    "grad_f = lambda x: -1/x+np.exp(x)\n",
    "\n",
    "\n",
    "decrement = lambda dx, x: (dx.dot(nabla_f(x).dot(dx)))**2/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, decrement: 120.223640\n",
      "Iteration: 1, decrement: 54.376955\n",
      "Iteration: 2, decrement: 27.246505\n",
      "Iteration: 3, decrement: 13.039832\n",
      "Iteration: 4, decrement: 4.269737\n",
      "Iteration: 5, decrement: 1.537255\n",
      "Iteration: 6, decrement: 1.047626\n",
      "Iteration: 7, decrement: 0.686185\n",
      "Iteration: 8, decrement: 0.226246\n",
      "Iteration: 9, decrement: 0.039570\n",
      "Iteration: 10, decrement: 0.002122\n",
      "Iteration: 11, decrement: 0.000006\n"
     ]
    }
   ],
   "source": [
    "from newton import newton_eq\n",
    "x = newton_eq(f, grad_f, nabla_f, x0, A=A, b=b, MAXITERS=100, TOL=1e-8,alpha = 0.01, beta = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXITERS = 20\n",
    "TOL = 1e-8\n",
    "alpha = 0.01\n",
    "beta = 0.5\n",
    "\n",
    "x = x0\n",
    "for iters in range(MAXITERS):\n",
    "    dx = np.linalg.solve(\n",
    "        np.block([[nabla_f(x), A.T], [A, np.zeros((m, m))]]),\n",
    "        np.block([-grad_f(x), np.zeros(m)]))\n",
    "    dx = dx[:n]\n",
    "    decrement_value = decrement(dx, x)/2\n",
    "    if decrement_value < TOL:\n",
    "        break\n",
    "    t = 1\n",
    "    \n",
    "    while f(x + t*dx) > f(x) - alpha * t * decrement_value:\n",
    "        t *= beta\n",
    "    \n",
    "    x += t*dx\n",
    "    print(\"Iteration: %d, decrement: %f\" % (iters, decrement_value))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs526",
   "language": "python",
   "name": "cs526"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
